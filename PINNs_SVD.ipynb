{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Inverse resolution of spatially varying diffusion coefficient using physics informed neural networks"
      ],
      "metadata": {
        "id": "4t4XKsBNFyzW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Resolving the diffusion coefficient is a key element in many biological and engineering systems, including pharmacological drug transport and fluid mechanics analyses. Here, we present an inverse solver that uses physics informed neural networks (PINNs) to calculate spatially-varying diffusion coefficients from image data.\n",
        "\n"
      ],
      "metadata": {
        "id": "4dZPnlwWF731"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We first load the dataset of images."
      ],
      "metadata": {
        "id": "Wsaqd04BGMi9"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IXARtxzCUZ0s",
        "outputId": "9e0d1827-b547-43b9-c0d0-dd857593a3a0"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TC4IGnAWx_Lc",
        "outputId": "01dd077e-a750-4b5f-8371-11bb72386982"
      },
      "source": [
        "%cd \"/content/drive/My Drive/PINN/PINNs-Sukirt/PINNs-master/main\"\n",
        "!ls './Data'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/PINN/PINNs-Sukirt/PINNs-master/main\n",
            "2D_constD_sol.mat\t\t  ehsanData_05_18.mat\t\t   imageData_tile6_06_03.mat\n",
            "2D_Dvar_sol.mat\t\t\t  ehsanData_05_26.mat\t\t   imageData_tile6_06_09_clean.mat\n",
            "2d_square_ResT_sol.mat\t\t  ehsanData_06_01_clean.mat\t   imageData_tile6_06_09.mat\n",
            "2D_varD_discrete.mat\t\t  ehsanData_06_01.mat\t\t   imageData_tile6_06_14.mat\n",
            "2D_VarD_Res_sol.mat\t\t  ehsanData_06_09_clean.mat\t   imageData_tile6_06_16.mat\n",
            "2D_varD_ResT_v2_sol.mat\t\t  gelatinImages.mat\t\t   imageData_tile6_entireDomain.mat\n",
            "2D_varD_ResT_v3_sol.mat\t\t  imageData_04_19.mat\t\t   images_01_29.mat\n",
            "2Dvar_tanh.mat\t\t\t  imageData_04_22.mat\t\t   images_08_09.mat\n",
            "AC.mat\t\t\t\t  imageData_04_26.mat\t\t   images_10_19.mat\n",
            "cylinder_nektar_t0_vorticity.mat  imageData_04_30.mat\t\t   images_11_08.mat\n",
            "cylinder_nektar_wake.mat\t  imageData_05_03.mat\t\t   images_11_17.mat\n",
            "ehsanData_05_15.mat\t\t  imageData_tile6_05_24.mat\t   KdV.mat\n",
            "ehsanData_05_16.mat\t\t  imageData_tile6_06_02.mat\t   KS.mat\n",
            "ehsanData_05_18_clean.mat\t  imageData_tile6_06_03_clean.mat  NLS.mat\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MxcAghYa-0DQ"
      },
      "source": [
        "### If you want to use wandb to track the results\n",
        "\n",
        "#pip install wandb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "We now import all the necessary libraries."
      ],
      "metadata": {
        "id": "0ylKmKM2Rqvz"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U4nG6qzsnHCB"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.io\n",
        "from scipy.interpolate import griddata\n",
        "import time\n",
        "import random\n",
        "from random import randint\n",
        "#import wandb\n",
        "import scipy.stats\n",
        "import math\n",
        "\n",
        "np.random.seed(1234)\n",
        "tf.random.set_seed(1234)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Function definitions\n",
        "\n",
        "We define our neural network class here."
      ],
      "metadata": {
        "id": "aCo96-IuSD_t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class NeuralNet(object):\n",
        "    def __init__(self, *inputs, layers):\n",
        "\n",
        "        self.layers = layers\n",
        "        self.num_layers = len(self.layers)\n",
        "\n",
        "        if len(inputs) == 0:\n",
        "            in_dim = self.layers[0]\n",
        "            self.X_mean = np.zeros([1, in_dim])\n",
        "            self.X_std = np.ones([1, in_dim])\n",
        "        else:\n",
        "            X = np.concatenate(inputs, 1)\n",
        "            self.X_mean = X.mean(0, keepdims=True)\n",
        "            self.X_std = X.std(0, keepdims=True)\n",
        "\n",
        "        self.weights = []\n",
        "        self.biases = []\n",
        "        self.gammas = []\n",
        "        initializer = tf.initializers.glorot_uniform()\n",
        "\n",
        "        for l in range(0,self.num_layers-1):\n",
        "            in_dim = self.layers[l]\n",
        "            out_dim = self.layers[l+1]\n",
        "            W = np.random.normal(size=[in_dim, out_dim])\n",
        "            b = np.zeros([1, out_dim])\n",
        "            g = np.ones([1, out_dim])\n",
        "            # tensorflow variables\n",
        "            self.weights.append(tf.Variable(W, dtype=tf.float32, trainable=True, name=str(random.randint(1,10000))))\n",
        "            self.biases.append(tf.Variable(b, dtype=tf.float32, trainable=True))\n",
        "            self.gammas.append(tf.Variable(g, dtype=tf.float32, trainable=True))\n",
        "\n",
        "    def get_trainable_variables(self):\n",
        "        return self.weights + self.biases + self.gammas\n",
        "\n",
        "    def __call__(self, *inputs):\n",
        "\n",
        "        H = (tf.concat(inputs, 1) - self.X_mean)/self.X_std\n",
        "\n",
        "        for l in range(0, self.num_layers-1):\n",
        "            W = self.weights[l]\n",
        "            b = self.biases[l]\n",
        "            g = self.gammas[l]\n",
        "            # weight normalization\n",
        "            V = W/tf.norm(W, axis = 0, keepdims=True)\n",
        "            # matrix multiplication\n",
        "            H = tf.matmul(H, V)\n",
        "            # add bias\n",
        "            H = g*H + b\n",
        "            # activation\n",
        "            if l < self.num_layers-2:\n",
        "                H = H*tf.sigmoid(H)\n",
        "\n",
        "        Y = tf.split(H, num_or_size_splits=H.shape[1], axis=1)\n",
        "\n",
        "        return Y\n"
      ],
      "metadata": {
        "id": "s83_3YcEDbAM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We define the functions to calculate the gradients, the relative error and the mean squared error.\n"
      ],
      "metadata": {
        "id": "Isw1K999kPY_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.custom_gradient\n",
        "def fwd_gradients(dy, dx):\n",
        "    def grad(dy_dx):\n",
        "        with tf.GradientTape() as tape:\n",
        "            tape.watch(dx)\n",
        "            G = tf.gradients(dy, dx, grad_ys=dy_dx)[0]\n",
        "        return G\n",
        "\n",
        "    return grad\n",
        "\n",
        "def relative_error(pred, exact):\n",
        "    if isinstance(pred, np.ndarray):\n",
        "        return np.sqrt(np.mean(np.square(pred - exact)) / np.mean(np.square(exact - np.mean(exact))))\n",
        "    return tf.sqrt(tf.reduce_mean(tf.square(pred - exact)) / tf.reduce_mean(tf.square(exact - tf.reduce_mean(exact))))\n",
        "\n",
        "def mean_squared_error(pred, exact):\n",
        "    if isinstance(pred, np.ndarray):\n",
        "        return np.mean(np.square(pred - exact))\n",
        "    return tf.reduce_mean(tf.square(pred - exact))\n"
      ],
      "metadata": {
        "id": "pTWeXmgkDrdu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Processing\n",
        "\n",
        "We create a dataset to train the neural network. We first read the file."
      ],
      "metadata": {
        "id": "Z89lyzL1STBU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Load data ###\n",
        "data = scipy.io.loadmat('./Data/Case_3.mat')\n",
        "\n",
        "Diff_star = data['Diff_star'] # N x T\n",
        "C_star = data['C_star'] # N x T\n",
        "t_star = data['t'].T # T x 1\n",
        "X_star = data['X_star'] # N x 2\n",
        "\n",
        "x_star = X_star[:,0:1]\n",
        "y_star = X_star[:,1:2]\n"
      ],
      "metadata": {
        "id": "D3vY-fkrEhGS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "And now, we process the data."
      ],
      "metadata": {
        "id": "DLmJL7J1Vf0b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "N = x_star.shape[0]\n",
        "T = t_star.shape[0]\n",
        "\n",
        "x_mesh = np.tile(x_star,(1,T)).flatten()[:,None]\n",
        "y_mesh = np.tile(y_star,(1,T)).flatten()[:,None]\n",
        "t_mesh = np.tile(t_star, (1,N)).T.flatten()[:,None]\n",
        "\n",
        "c_mesh_data = C_star.flatten()[:,None]\n",
        "d_mesh_data = Diff_star.flatten()[:,None]\n",
        "\n",
        "idx_x = np.random.choice( x_mesh.shape[0], x_mesh.shape[0], replace=False)      # Randomly select data points\n",
        "\n",
        "x_data = np.float32(x_mesh[idx_x,:])\n",
        "y_data = np.float32(y_mesh[idx_x,:])\n",
        "t_data = np.float32(t_mesh[idx_x,:])\n",
        "\n",
        "c_data = np.float32(c_mesh_data[idx_x,:])\n",
        "d_data = np.float32(d_mesh_data[idx_x,:])\n",
        "\n",
        "dt = (t_star[1] - t_star[0])\n",
        "\n",
        "# Randomize dataset\n",
        "idx = np.random.choice(N*T, N*T, replace=False)\n",
        "idx_test = np.random.choice(N*T, int(0.1*N*T), replace=False)\n",
        "\n",
        "t_eqns = np.float32(np.random.uniform(low=t_data.min(), high=t_data.max(), size=(256000000,t_data.shape[1])))\n",
        "x_eqns = np.float32(np.random.uniform(low=x_data.min(), high=x_data.max(), size=(256000000,t_data.shape[1])))\n",
        "y_eqns = np.float32(np.random.uniform(low=y_data.min(), high=y_data.max(), size=(256000000,t_data.shape[1])))\n",
        "\n",
        "t_test_set = np.float32(t_mesh[idx_test,:])\n",
        "x_test_set = np.float32(x_mesh[idx_test,:])\n",
        "y_test_set = np.float32(y_mesh[idx_test,:])\n",
        "c_test_set = np.float32(c_mesh_data[idx_test,:])\n",
        "d_test_set = np.float32(d_mesh_data[idx_test,:])"
      ],
      "metadata": {
        "id": "-1b09yHcVlIl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We define tensorflow dataset iterators here to train the neural network efficiently."
      ],
      "metadata": {
        "id": "I5BlpaRjV44C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset creation using tf.data\n",
        "batch_size = 256\n",
        "\n",
        "# Dataset initialization\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_data, y_data, t_data, c_data, d_data))\n",
        "train_dataset = train_dataset.shuffle(buffer_size=10).repeat().batch(batch_size, drop_remainder=True)\n",
        "train_iterator = iter(train_dataset)\n",
        "\n",
        "eqns_dataset = tf.data.Dataset.from_tensor_slices((x_eqns, y_eqns, t_eqns))\n",
        "eqns_dataset = eqns_dataset.shuffle(buffer_size=10).repeat().batch(batch_size, drop_remainder=True)\n",
        "eqns_iterator = iter(eqns_dataset)\n"
      ],
      "metadata": {
        "id": "RpuEcZ9YE7MM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create the neural networks\n",
        "\n"
      ],
      "metadata": {
        "id": "tPyo18RmSjZn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We define the hyperparameters for the neural networks here."
      ],
      "metadata": {
        "id": "pGoEEWvPYC67"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create neural networks\n",
        "layers = [3] + 8 * [64] + [1]\n",
        "layers_D = [2] + 2 * [8] + [1]\n",
        "c_net = NeuralNet(x_data, y_data, t_data, layers = layers)\n",
        "D_net = NeuralNet(x_data, y_data, layers = layers_D)"
      ],
      "metadata": {
        "id": "c50zIC61EZDh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "it = 0\n",
        "N_iter = 320000  #Number of iterations\n",
        "\n",
        "eta_min = 2.5e-6  #Minimum value of the learning rate\n",
        "eta_max = 2.5e-3  #Maximum value of the learning rate\n",
        "\n",
        "\n",
        "start_time = time.time()\n",
        "running_time = 0"
      ],
      "metadata": {
        "id": "hakRI5lZMvyq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the network"
      ],
      "metadata": {
        "id": "MuGhcaxuYPJn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "@tf.function\n",
        "def compute_gradients(x_data_tf, y_data_tf, t_data_tf, c_data_tf, d_data_tf, x_eqns_tf, y_eqns_tf, t_eqns_tf, dt):\n",
        "    with tf.GradientTape(persistent=True) as tape:\n",
        "        tape.watch([x_eqns_tf, y_eqns_tf])\n",
        "        dt = tf.cast(dt, tf.float32)\n",
        "\n",
        "        c_data_pu_1, c_eqns_pu_1, c_eqns_pu_2, D_pred_, D_pred_data = c_net(x_data_tf, y_data_tf, t_data_tf)[0], \\\n",
        "                                                                     c_net(x_eqns_tf, y_eqns_tf, t_eqns_tf)[0], \\\n",
        "                                                                     c_net(x_eqns_tf, y_eqns_tf, t_eqns_tf + dt)[0], \\\n",
        "                                                                     D_net(x_eqns_tf, y_eqns_tf)[0], \\\n",
        "                                                                     D_net(x_data_tf, y_data_tf)[0]\n",
        "\n",
        "        c_x = tape.gradient([c_eqns_pu_2], [x_eqns_tf])[0]\n",
        "        c_y = tape.gradient([c_eqns_pu_2], [y_eqns_tf])[0]\n",
        "        D_x = tape.gradient([D_pred_], [x_eqns_tf])[0]\n",
        "        D_y = tape.gradient([D_pred_], [y_eqns_tf])[0]\n",
        "        c_xx = tape.gradient([c_x], [x_eqns_tf])[0]\n",
        "        c_yy = tape.gradient([c_y], [y_eqns_tf])[0]\n",
        "\n",
        "        f_c = D_pred_ * (c_xx + c_yy) + D_x * c_x + D_y * c_y\n",
        "        c_eqns_pi_1 = c_eqns_pu_2 - dt * f_c\n",
        "\n",
        "        scale_c = np.std(c_data)\n",
        "        scale_d = np.std(d_data)\n",
        "\n",
        "        loss_data = mean_squared_error(c_data_pu_1 / scale_c, c_data_tf / scale_c)\n",
        "        loss_diff = mean_squared_error(D_pred_data / scale_d, d_data_tf / scale_d)\n",
        "        loss_consistency = mean_squared_error(c_eqns_pu_1 / scale_c, c_eqns_pi_1 / scale_c)\n",
        "\n",
        "        total_loss = loss_data + loss_consistency\n",
        "\n",
        "    gradients = tape.gradient(total_loss, c_net.get_trainable_variables() + D_net.get_trainable_variables())\n",
        "    optimizer.apply_gradients(zip(gradients, c_net.get_trainable_variables() + D_net.get_trainable_variables()))\n",
        "\n",
        "    return loss_data, loss_diff, loss_consistency\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qB8OU41dRQvo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is the training loop."
      ],
      "metadata": {
        "id": "LkCHvBM9YY37"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "while it < N_iter:\n",
        "    lr = eta_min + 0.5 * (eta_max - eta_min) * (1 + np.cos(math.pi * it / N_iter))\n",
        "\n",
        "    optimizer.learning_rate.assign(lr)\n",
        "    x_data_tf, y_data_tf, t_data_tf, c_data_tf, d_data_tf = next(train_iterator)\n",
        "    x_eqns_tf, y_eqns_tf, t_eqns_tf = next(eqns_iterator)\n",
        "\n",
        "    # Training step\n",
        "    loss_data, loss_diff, loss_consistency = compute_gradients( x_data_tf, y_data_tf, t_data_tf, c_data_tf, d_data_tf,\n",
        "                                                              x_eqns_tf, y_eqns_tf, t_eqns_tf, dt)\n",
        "\n",
        "    # Logging and progress monitoring\n",
        "    if it % 10 == 0:\n",
        "        elapsed = time.time() - start_time\n",
        "        running_time += elapsed / 3600.0\n",
        "        print(f\"It: {it}, Loss_data: {loss_data.numpy():.3e}, Loss_f: {loss_consistency.numpy():.3e}, \"\n",
        "              f\"Loss_diff: {loss_diff.numpy():.3e}, Time: {elapsed:.2f}s, Running Time: {running_time:.2f}h, \"\n",
        "              f\"Learning Rate: {lr:.1e}\")\n",
        "        # Log using WandB or other logging method\n",
        "        #wandb.log({'It': it, 'loss_d': loss_data.numpy(), 'loss_con': loss_consistency.numpy(),'loss_diff': loss_diff.numpy(),  'LR':lr})\n",
        "        start_time = time.time()\n",
        "    it += 1\n",
        "    if it % 1000 == 0:\n",
        "        c_star_val, d_star_val = c_net(x_test_set, y_test_set, t_test_set), D_net(x_test_set, y_test_set)\n",
        "        #wandb.log({'Error_c':relative_error(c_star_val, c_test_set), 'Error_d':relative_error(d_star_val, d_test_set) })"
      ],
      "metadata": {
        "id": "fwVKISDKYYLB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prediction and plotting\n",
        "\n",
        "Once the network is trained, we plot the results."
      ],
      "metadata": {
        "id": "_Lcrmab4Sy_a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "c_star_val, d_star_val = c_net(x_test_set, y_test_set, t_test_set), D_net(x_test_set, y_test_set)\n",
        "print(relative_error(c_star_val, c_test_set))\n",
        "print(relative_error(d_star_val, d_test_set))"
      ],
      "metadata": {
        "id": "BIRdqhdEV7bV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "C_pred_val = np.zeros_like(C_star)\n",
        "D_star_val = D_net(np.float32(X_star[:,0:1]), np.float32(X_star[:,1:2]))[0]\n",
        "\n",
        "TT = np.tile(t_star, (1,N)).T\n",
        "\n",
        "for snp in range(C_star.shape[1]-1):\n",
        "    snap = np.array([snp])\n",
        "    c_star_val =  c_net(np.float32(X_star[:,0:1]), np.float32(X_star[:,1:2]), np.float32(TT[:,snap]))[0]\n",
        "    C_pred_val[:,snap] = c_star_val\n",
        "\n"
      ],
      "metadata": {
        "id": "JYHRWzxKbaHY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict for plotting\n",
        "lb = X_star.min(0)\n",
        "ub = X_star.max(0)\n",
        "nn = 200\n",
        "x = np.linspace(lb[0], ub[0], nn)\n",
        "y = np.linspace(lb[1], ub[1], nn)\n",
        "X, Y = np.meshgrid(x,y)\n"
      ],
      "metadata": {
        "id": "iB7p0yJdLRo1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DIFF_star = griddata(X_star, Diff_star[:,0].flatten(), (X, Y), method='cubic')\n",
        "d_min = np.min(DIFF_star.flatten())\n",
        "d_max = np.max(DIFF_star.flatten())\n",
        "levels_diff = np.arange(d_min,d_max,(d_max-d_min)/100)\n",
        "\n",
        "\n",
        "extend_diff = \"both\"\n",
        "cmap = plt.cm.get_cmap(\"seismic\")\n"
      ],
      "metadata": {
        "id": "KeVaYm5CK9fe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig3, ax3 = plt.subplots()\n",
        "CS3 = ax3.contourf(X, Y, (DIFF_star), levels=levels_diff, cmap=cmap,extend=extend_diff)\n",
        "ax3.set_title(r'Diffusion coefficient')# $[\\frac{m^2}{s}]$')\n",
        "ax3.set_aspect('equal')\n",
        "ax3.set_xlabel('X')\n",
        "ax3.set_ylabel('Y')\n",
        "cbar = plt.colorbar(CS3,) #, format=ticker.FuncFormatter(fmt))\n",
        "cbar.set_label(r'$\\frac{m^2}{s}$',rotation=-0, labelpad=10,size=15)\n",
        "plt.gca().invert_yaxis()\n"
      ],
      "metadata": {
        "id": "dHY1AlowLXEQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DIFF_pred = griddata(X_star, D_star_val.numpy().flatten(), (X, Y), method='cubic')"
      ],
      "metadata": {
        "id": "xgyTLP3XLb9H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d_min = np.min(DIFF_pred.flatten())\n",
        "d_max = np.max(DIFF_pred.flatten())\n",
        "levels_diff = np.arange(d_min,d_max,(d_max-d_min)/100)\n",
        "\n",
        "fig3, ax3 = plt.subplots()\n",
        "CS3 = ax3.contourf(X, Y, (DIFF_pred), levels=levels_diff, cmap=cmap,extend=extend_diff)\n",
        "ax3.set_title(r'Diffusion coefficient')# $[\\frac{m^2}{s}]$')\n",
        "ax3.set_aspect('equal')\n",
        "ax3.set_xlabel('X')\n",
        "ax3.set_ylabel('Y')\n",
        "cbar = plt.colorbar(CS3,) #, format=ticker.FuncFormatter(fmt))\n",
        "cbar.set_label(r'$\\frac{m^2}{s}$',rotation=-0, labelpad=10,size=15)\n",
        "plt.gca().invert_yaxis()\n"
      ],
      "metadata": {
        "id": "bduNjBOqLqIx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}